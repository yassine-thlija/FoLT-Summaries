{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FoLT Tutorial 2 Summary (Preprocessing):\n",
    "### Part1: Regular Expressions\n",
    "**Functions:**\n",
    "1. `re.findall(pattern, word)`: This functions returns a list of all occurences of pattern in word\n",
    "- Some rules:\n",
    "    + $ stands for the end of the word\n",
    "    + ^ stands for the ending of the words\n",
    "    + [xyz] means either x, y, or z (works also for numbers, see example 2)\n",
    "    + expr{x} means the expression has to be repeated twice (see example 2) \n",
    "- Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ere']\n",
      "['12', '34', '12']\n",
      "[]\n",
      "['there']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "word = 'hello again 12345 123 hello there'\n",
    "print(re.findall('ere$', word))\n",
    "print(re.findall('[0-7]{2}', word))\n",
    "print(re.findall('^[ghi][def][jlk][mno]$', word))\n",
    "print(re.findall('[tuv][ghi][def][pqrs][def]$', word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. `re.split(pattern, word)`: This expression splits word using the pattern and returns the result as a list of strings\n",
    "- Some Rules:\n",
    "    + If the pattern isn't found, split returns a list of the original string\n",
    "    + If you use parantheses in the pattern, the pattern is also returned in the list\n",
    "- Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HEYYYYY babe']\n",
      "['Hello', 'World', 'Hello', 'Universe']\n",
      "['Hello', '-', 'World', '-', 'Hello', '-', 'Universe']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "text = \"Hello-World-Hello-Universe\"\n",
    "pattern1 = \"-\"\n",
    "pattern2 = \"(-)\"\n",
    "split_text = re.split(pattern1, text)\n",
    "print(split_text)\n",
    "split_text = re.split(pattern2, text)\n",
    "print(split_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. `re.compile(word)` compiles a regex pattern into an object, enabling efficient reuse for matching operations in text.\n",
    "- Some Rules:\n",
    "    + It returns an RE object which we can use to perform patter matching\n",
    "    + The compiled object can be reused multiple times without recompiling the pattern\n",
    "    + You can use some flags like `re.IGNORECASE` (see example)\n",
    "- Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('hello', re.IGNORECASE)\n",
    "\n",
    "matches = pattern.findall('Hello World, hello universe')\n",
    "\n",
    "print(matches)  # Output: ['Hello', 'hello']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### USEFUL IMPORTS FOR PART 2, MUST EXECUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 2: NLTK \n",
    "- Basic preprocessing with NLTK\n",
    "    + Two Important datasets:\n",
    "        1. **stopwords**: common words in various langauges that we usually filter out, in english for example: \"the\", \"is\", \"in\", \"for, \"where\", etc.\n",
    "        2. **wordnet**: a large lexaical database of english words, which are linked together by their semantic relationships, like synonyms, antonyms, definitions...\n",
    "    + Example of usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download stopwords and wordnet\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# define text to preprocess\n",
    "text = \"This is an example sentence to demonstrate text preprocessing with NLTK. We will tokenize, remove stopwords, and stem the words.\"\n",
    "\n",
    "# tokenize text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# stem words\n",
    "ps = PorterStemmer()\n",
    "stemmed_tokens = [ps.stem(token) for token in filtered_tokens]\n",
    "\n",
    "# lemmatize words\n",
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_tokens = [wnl.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "print(\"Original text:\", text)\n",
    "print(\"Tokenized text:\", tokens)\n",
    "print(\"Filtered text (stopwords removed):\", filtered_tokens)\n",
    "print(\"Stemmed text:\", stemmed_tokens)\n",
    "print(\"Lemmatized text:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Segmentation using NLTK:\n",
    "    + We are gonna use **punkt** dataset, this is well-suited for dividing plain text into sentences\n",
    "    + Example of sentence segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text = \"This is a sentence. This is another sentence. This is a third sentence.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we are gonna use the brown corpus (it is made of a collection of text samples with over a million word) to write a function called `word_freq(word, category)` that takes a token and a category, it calculates the frequency of that word in a distinct category of the brown corpus\n",
    "    + Some info about the function: The `nltk.FreqDist()` function in the Natural Language Toolkit (NLTK) library in Python is used to create a frequency distribution of words, i.e., it counts the frequency of each vocabulary item in the text. \n",
    "    + Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "def word_freq(word, category):\n",
    "\n",
    "    text = brown.words(categories=category)\n",
    "    fdist = nltk.FreqDist(text)\n",
    "    return fdist[word]\n",
    "\n",
    "print(word_freq('good', 'news')) \n",
    "print(word_freq('good', 'hobbies')) \n",
    "print(word_freq('good', 'belles_lettres')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We used `nltk.FreqDist()` to calculate the frequency distribution of words in a text, now we are gonna use a similar function that calcuate the distribution conditionally: `nltk.ConditionalFreqDist()`, in the next example we are gonna make function that uses pair of `(gender, name)` as its condition then draw a graph to display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Download corpus if you haven't already\n",
    "# nltk.download('names')\n",
    "from nltk.corpus import names\n",
    "# Your code here\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (gender, name[-1])\n",
    "        for gender in names.fileids()\n",
    "        for name in names.words(gender)\n",
    "    )\n",
    "\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART 3: spaCy\n",
    "- The advantage of the spaCy library is that it has a wide variety of languages.\n",
    "- When using spaCy we have to specify which language we want to work with, as it doesn't assume it \n",
    "- Now we are gonna make a pipeline called an NLP pipeline which tokenize, tag, parse and name entity recognize. Execute the following code to understand wht actually happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "raw = \"Hard to judge whether these sides were good. We were grossed \" \\\n",
    "      \"out by the melted styrofoam and didn't want to eat it for fear of getting sick.\"\n",
    "\n",
    "doc = nlp(raw)\n",
    "\n",
    "print(doc) #After the pipeline is complete, doc contains the processed text.\n",
    "\n",
    "for token in doc: #Iterate over the tokens in the doc\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now to try other languages other than englis, we are gonna use german spacy library (we are gonna do the same thing with the pipeline, just different language)\n",
    "- We can either import `spacy.lang.de` or load the german model using `spacy.load(\"de_core_news_sm\")`\n",
    "- Code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#!python -m spacy download de_core_news_sm\n",
    "\n",
    "german_raw = \"Schwer zu beurteilen, ob diese Seiten gut waren. Wir haben uns \" \\\n",
    "            \"vor dem geschmolzenen Styropor geekelt und wollten es nicht essen, aus Angst, krank zu werden.\"\n",
    "\n",
    "nlp_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# Your code here\n",
    "doc_de = nlp_de(german_raw)\n",
    "\n",
    "print(doc_de)\n",
    "\n",
    "for token in doc_de:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doc for indexing, spacy has the same indexing as base python, see example(we created doc earlier before, we are just gonna reuse it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_word = doc[-1]\n",
    "first_word = doc[0]\n",
    "print(first_word, last_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spacy can detect the lexical attribute of a token (this already happened in the pipelining), now we are gonna test that using `is_digit` and `is_punct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(first_word.is_digit) #Hard\n",
    "print(last_word.is_punct) # ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sentence Segmentation using spaCy, we are gonna use the `sents` attributes of the `Doc` object to obtain the list of sentences, the list is of type generator so we have to iterate through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.pipeline import Sentencizer\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe('sentencizer')  # Add sentencizer to the pipeline\n",
    "\n",
    "text = \"This is the first sentence. This is the second sentence. And this is the third sentence.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
